{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a898bcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "from functools import reduce\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7c2032af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not read: hilbilly.wri\n",
      "File not read: howlong.hum\n",
      "File not read: oxymoron.txt\n",
      "File not read: steroid.txt\n",
      "File not read: various.txt\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "docnames = []\n",
    "\n",
    "for filename in os.listdir('data'):\n",
    "    try:\n",
    "        lines = []\n",
    "        file = open('data/'+filename, 'r')\n",
    "        for i in file:\n",
    "            lines.append(i.replace('\\n', ''))\n",
    "        docnames.append(filename)\n",
    "        docs.append(lines)\n",
    "    except:\n",
    "        print('File not read:', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ae80ce18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1128"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467723a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3a46f66",
   "metadata": {},
   "source": [
    "## A: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1791fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6920d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    procd = []\n",
    "    for line in doc:\n",
    "        line = line.lower()\n",
    "        line = word_tokenize(line)\n",
    "        tokens = []\n",
    "        for word in line:\n",
    "            if(word not in stop_words and word.isalnum()):\n",
    "                procd.append(stemmer.stem(word))\n",
    "                #procd.append(lemmatizer.lemmatize(word))\n",
    "                #procd.append(word)\n",
    "    return procd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "38b14116",
   "metadata": {},
   "outputs": [],
   "source": [
    "prodocs = []\n",
    "\n",
    "for doc in docs:\n",
    "    procd = preprocess(doc)\n",
    "    prodocs.append(procd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf50a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4db23326",
   "metadata": {},
   "source": [
    "## B: Unigram Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a47fa698",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = defaultdict(list)\n",
    "\n",
    "for i, doc in enumerate(prodocs):\n",
    "    for j, word in enumerate(doc):\n",
    "        if word in index:\n",
    "            if i not in index[word][1]:\n",
    "                index[word][1].append(i)\n",
    "            index[word][0] = len(index[word][1])\n",
    "        else:\n",
    "            index[word] = [1]\n",
    "            index[word].append([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "afe1ee4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24,\n",
       " [30,\n",
       "  128,\n",
       "  200,\n",
       "  240,\n",
       "  252,\n",
       "  295,\n",
       "  406,\n",
       "  412,\n",
       "  450,\n",
       "  509,\n",
       "  517,\n",
       "  518,\n",
       "  600,\n",
       "  669,\n",
       "  720,\n",
       "  751,\n",
       "  784,\n",
       "  812,\n",
       "  814,\n",
       "  847,\n",
       "  912,\n",
       "  921,\n",
       "  1031,\n",
       "  1070]]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index['rainbow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c95cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1136b80a",
   "metadata": {},
   "source": [
    "## C: Queries Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e549ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ander(l1, l2):\n",
    "    if(len(l2) < len(l1)):\n",
    "        l1, l2 = l2, l1\n",
    "    ret = []\n",
    "    comparisons = 0\n",
    "    i = j = 0\n",
    "    \n",
    "    while(i<len(l1) and j<len(l2)):\n",
    "        if(l1[i] == l2[j]):\n",
    "            ret.append(l1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif(l1[i] < l2[j]):\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "        comparisons += 1\n",
    "    \n",
    "    return comparisons, ret\n",
    "\n",
    "def orer(l1, l2):\n",
    "    ret = []\n",
    "    comparisons = 0\n",
    "    i = j = 0\n",
    "    \n",
    "    while(i<len(l1) and j<len(l2)):\n",
    "        if(l1[i] == l2[j]):\n",
    "            ret.append(l1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif(l1[i] < l2[j]):\n",
    "            ret.append(l1[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            ret.append(l2[j])\n",
    "            j += 1\n",
    "        comparisons += 1\n",
    "    \n",
    "    while(i<len(l1)):\n",
    "        ret.append(l1[i])\n",
    "        i += 1\n",
    "    while(j<len(l2)):\n",
    "        ret.append(l2[j])\n",
    "        j += 1\n",
    "    \n",
    "    return comparisons, ret\n",
    "\n",
    "def noter(l1):\n",
    "    idlist = list(range(len(prodocs)))\n",
    "    for docid in l1:\n",
    "        idlist.remove(docid)\n",
    "    return idlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5fc7e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle(l1, l2, op):\n",
    "    if(op == 'and'):\n",
    "        return ander(l1, l2)\n",
    "    elif(op == 'or'):\n",
    "        return orer(l1, l2)\n",
    "    elif(op == 'or not'):\n",
    "        return orer(l1, noter(l2))\n",
    "    elif(op == 'and not'):\n",
    "        return ander(l1, noter(l2))\n",
    "    else:\n",
    "        print('Invalid operation')\n",
    "        return -1, []\n",
    "\n",
    "def process(query, ops):\n",
    "    tokens = preprocess([query])\n",
    "    ops = [op.lower().strip() for op in ops]\n",
    "    print('Query tokens:', tokens)\n",
    "    print('Operations:', ops)\n",
    "    comparisons = 0\n",
    "    \n",
    "    # Input is assumed to be in correct format\n",
    "    if(tokens and ops):\n",
    "        l1 = index[tokens[0]][1]\n",
    "        for i in range(len(ops)):\n",
    "            l2 = index[tokens[i+1]][1]\n",
    "            numc, l1 = handle(l1, l2, ops[i])\n",
    "            if(numc == -1):\n",
    "                return\n",
    "            comparisons += numc\n",
    "        \n",
    "        return comparisons, l1\n",
    "    else:\n",
    "        print('Empty input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ccd64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815cd4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e69db9e",
   "metadata": {},
   "source": [
    "## D: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2a584996",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'lion stood thoughtfully for a moment'\n",
    "ops = ['or', 'or', 'or']\n",
    "ops1 = ['or', 'and', 'or not']\n",
    "ops2 = ['or', 'or', 'and not']\n",
    "ops3 = ['or', 'and', 'or not']\n",
    "ops4 = ['or', 'or', 'or not']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "83fcb7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query tokens: ['lion', 'stood', 'thought', 'moment']\n",
      "Operations: ['or', 'or', 'or']\n",
      "860 408\n"
     ]
    }
   ],
   "source": [
    "a = process(query, ops)\n",
    "print(a[0], len(a[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6dd298ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query tokens: ['lion', 'stood', 'thought', 'moment']\n",
      "Operations: ['or', 'and', 'or not']\n",
      "1455 1013\n"
     ]
    }
   ],
   "source": [
    "a = process(query, ops1)\n",
    "print(a[0], len(a[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4bba53dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query tokens: ['lion', 'stood', 'thought', 'moment']\n",
      "Operations: ['or', 'or', 'and not']\n",
      "1530 253\n"
     ]
    }
   ],
   "source": [
    "a = process(query, ops2)\n",
    "print(a[0], len(a[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3a28a8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query tokens: ['lion', 'stood', 'thought', 'moment']\n",
      "Operations: ['or', 'and', 'or not']\n",
      "1455 1013\n"
     ]
    }
   ],
   "source": [
    "a = process(query, ops3)\n",
    "print(a[0], len(a[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7e4f5cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query tokens: ['lion', 'stood', 'thought', 'moment']\n",
      "Operations: ['or', 'or', 'or not']\n",
      "1530 1081\n"
     ]
    }
   ],
   "source": [
    "a = process(query, ops4)\n",
    "print(a[0], len(a[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11626b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7c019448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the query: telephone paved roads\n",
      "\n",
      "Enter operations separated by \",\",\n",
      "\n",
      "Enter the operations : or not, and not\n"
     ]
    }
   ],
   "source": [
    "input_query = input('Enter the query: ')\n",
    "print('\\nEnter operations separated by \",\",')\n",
    "input_ops = list(map(str,input(\"\\nEnter the operations : \").strip().split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "99d9aad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query tokens: ['telephon', 'pave', 'road']\n",
      "Operations: ['or not', 'and not']\n",
      "\n",
      "Number of documents matched: 999\n",
      "Number of comparisons: 2239\n",
      "\n",
      "Document list\n",
      "\n",
      "1st_aid.txt\n",
      "abbott.txt\n",
      "acetab1.txt\n",
      "acne1.txt\n",
      "acronym.txt\n",
      "adameve.hum\n",
      "adcopy.hum\n",
      "addrmeri.txt\n",
      "admin.txt\n",
      "adrian_e.faq\n",
      "ads.txt\n",
      "adt_miam.txt\n",
      "advrtize.txt\n",
      "aeonint.txt\n",
      "age.txt\n",
      "aggie.txt\n",
      "airlines\n",
      "alabama.txt\n",
      "alcatax.txt\n",
      "alcohol.hum\n",
      "alflog.txt\n",
      "allusion\n",
      "all_grai\n",
      "ambrose.bie\n",
      "amchap2.txt\n",
      "analogy.hum\n",
      "aniherb.txt\n",
      "anime.lif\n",
      "anim_lif.txt\n",
      "annoy.fascist\n",
      "anorexia.txt\n",
      "answers\n",
      "anthropo.stu\n",
      "antibiot.txt\n",
      "antimead.bev\n",
      "aphrodis.txt\n",
      "appbred.brd\n",
      "appetiz.rcp\n",
      "applepie.des\n",
      "apsaucke.des\n",
      "apsnet.txt\n",
      "arab.dic\n",
      "arcadian.txt\n",
      "argotdic.txt\n",
      "arnold.txt\n",
      "art-fart.hum\n",
      "arthriti.txt\n",
      "atherosc.txt\n",
      "atombomb.hum\n",
      "att.txt\n",
      "aussie.lng\n",
      "avengers.lis\n",
      "awespinh.sal\n",
      "ayurved.txt\n",
      "a_fish_c.apo\n",
      "a_tv_t-p.com\n",
      "b-2.jok\n",
      "b12.txt\n",
      "back1.txt\n",
      "bad\n",
      "bad-d\n",
      "bad.jok\n",
      "badday.hum\n",
      "bagelope.txt\n",
      "bakebred.txt\n",
      "baklava.des\n",
      "banana01.brd\n",
      "banana02.brd\n",
      "banana03.brd\n",
      "banana04.brd\n",
      "banana05.brd\n",
      "bank.rob\n",
      "barney.cn1\n",
      "basehead.txt\n",
      "batrbred.txt\n",
      "bb\n",
      "bbc_vide.cat\n",
      "bbq.txt\n",
      "beapimp.hum\n",
      "beave.hum\n",
      "beer-g\n",
      "beer-gui\n",
      "beer.gam\n",
      "beer.hum\n",
      "beer.txt\n",
      "beerdiag.txt\n",
      "beergame.hum\n",
      "beergame.txt\n",
      "beerjesus.hum\n",
      "beershrm.fis\n",
      "beershrp.fis\n",
      "beerwarn.txt\n",
      "beesherb.txt\n",
      "beginn.ers\n",
      "berryeto.bev\n",
      "bhang.fun\n",
      "bhb.ill\n",
      "bible.txt\n",
      "bigpic1.hum\n",
      "billcat.hum\n",
      "bimg.prn\n",
      "bingbong.hum\n",
      "bitchcar.hum\n",
      "blackapp.hum\n",
      "blackhol.hum\n",
      "blake7.lis\n",
      "blaster.hum\n",
      "bless.bc\n",
      "blkbean.txt\n",
      "blkbnsrc.vgn\n",
      "blood.txt\n",
      "blooprs1.asc\n",
      "bmdn01.txt\n",
      "bnb_quot.txt\n",
      "boarchil.txt\n",
      "boatmemo.jok\n",
      "boe.hum\n",
      "bond-2.txt\n",
      "boneles2.txt\n",
      "booknuti.txt\n",
      "booze.fun\n",
      "booze1.fun\n",
      "booze2.fun\n",
      "bored.txt\n",
      "boston.geog\n",
      "bozo_tv.leg\n",
      "brainect.hum\n",
      "brdpudd.des\n",
      "bread.rec\n",
      "bread.txt\n",
      "breadpud.des\n",
      "bredcake.des\n",
      "browneco.hum\n",
      "brownie.rec\n",
      "brush1.txt\n",
      "btaco.txt\n",
      "btcisfre.hum\n",
      "btscke01.des\n",
      "btscke02.des\n",
      "btscke03.des\n",
      "btscke04.des\n",
      "btscke05.des\n",
      "buffwing.pol\n",
      "bugbreak.hum\n",
      "bugs.txt\n",
      "buldrwho.txt\n",
      "bunacald.fis\n",
      "burrito.mea\n",
      "butcher.txt\n",
      "butstcod.fis\n",
      "butwrong.hum\n",
      "buzzword.hum\n",
      "bw-phwan.hat\n",
      "bw-summe.hat\n",
      "bw.txt\n",
      "byfb.txt\n",
      "c0dez.txt\n",
      "cabbage.txt\n",
      "caesardr.sal\n",
      "cake.rec\n",
      "calamus.hrb\n",
      "calculus.txt\n",
      "calif.hum\n",
      "calvin.txt\n",
      "cancer.rat\n",
      "candybar.fun\n",
      "capital.txt\n",
      "caramels.des\n",
      "carowner.txt\n",
      "cartoon.law\n",
      "cartoon.laws\n",
      "cartoon_.txt\n",
      "cartoon_laws.txt\n",
      "cartwb.son\n",
      "cast.lis\n",
      "catballs.hum\n",
      "catin.hat\n",
      "catranch.hum\n",
      "catstory.txt\n",
      "cbmatic.hum\n",
      "cereal.txt\n",
      "cform2.txt\n",
      "cgs_lst.txt\n",
      "chainltr.txt\n",
      "change.hum\n",
      "charity.hum\n",
      "cheapfar.hum\n",
      "cheapin.la\n",
      "chickenheadbbs.txt\n",
      "chickens.jok\n",
      "chickens.txt\n",
      "childhoo.jok\n",
      "childrenbooks.txt\n",
      "chili.txt\n",
      "chinese.txt\n",
      "chinesec.hum\n",
      "choco-ch.ips\n",
      "christop.int\n",
      "chung.iv\n",
      "chunnel.txt\n",
      "church.sto\n",
      "clancy.txt\n",
      "classicm.hum\n",
      "climbing.let\n",
      "cmu.share\n",
      "co-car.jok\n",
      "cockney.alp\n",
      "coffee.faq\n",
      "coffee.txt\n",
      "coffeebeerwomen.txt\n",
      "cogdis.txt\n",
      "coke.fun\n",
      "coke.txt\n",
      "coke1\n",
      "cokeform.txt\n",
      "coke_fan.naz\n",
      "coladrik.fun\n",
      "coladrik.txt\n",
      "cold.fus\n",
      "coldfake.hum\n",
      "college.hum\n",
      "college.sla\n",
      "college.txt\n",
      "computer.txt\n",
      "comrevi1.hum\n",
      "conan.txt\n",
      "confucius_say.txt\n",
      "contract.moo\n",
      "cookberk\n",
      "cookbkly.how\n",
      "cooking.fun\n",
      "cooking.jok\n",
      "coollngo2.txt\n",
      "cooplaws\n",
      "cops.txt\n",
      "corporat.txt\n",
      "court.quips\n",
      "cowexplo.hum\n",
      "coyote.txt\n",
      "crazy.txt\n",
      "critic.txt\n",
      "crzycred.lst\n",
      "cucumber.jok\n",
      "cucumber.txt\n",
      "cuisine.txt\n",
      "curiousgeorgie.txt\n",
      "curry.hrb\n",
      "curry.txt\n",
      "curse.txt\n",
      "cybrtrsh.txt\n",
      "d-ned.hum\n",
      "dalive\n",
      "damiana.hrb\n",
      "dandwine.bev\n",
      "dark.suc\n",
      "dead-r\n",
      "dead4.txt\n",
      "deadlysins.txt\n",
      "deathhem.txt\n",
      "deep.txt\n",
      "defectiv.hum\n",
      "desk.txt\n",
      "deterior.hum\n",
      "devils.jok\n",
      "diesmurf.txt\n",
      "diet.txt\n",
      "dieter.txt\n",
      "dingding.hum\n",
      "dining.out\n",
      "disaster.hum\n",
      "disclmr.txt\n",
      "disclym.txt\n",
      "doc-says.txt\n",
      "docdict.txt\n",
      "docspeak.txt\n",
      "doggun.sto\n",
      "donut.txt\n",
      "dover.poem\n",
      "draxamus.txt\n",
      "drinker.txt\n",
      "drinking.tro\n",
      "drinkrul.jok\n",
      "drinks.txt\n",
      "dromes.txt\n",
      "druggame.hum\n",
      "drugshum.hum\n",
      "drunk.txt\n",
      "dthought.txt\n",
      "dym\n",
      "eandb.drx\n",
      "earp\n",
      "eatme.txt\n",
      "econridl.fun\n",
      "egg-bred.txt\n",
      "egglentl.vgn\n",
      "eggroll1.mea\n",
      "electric.txt\n",
      "element.jok\n",
      "elephant.fun\n",
      "elevator.fun\n",
      "empeval.txt\n",
      "engineer.hum\n",
      "english\n",
      "english.txt\n",
      "engmuffn.txt\n",
      "engrhyme.txt\n",
      "enlightenment.txt\n",
      "epikarat.txt\n",
      "epiquest.txt\n",
      "episimp2.txt\n",
      "epitaph\n",
      "epi_merm.txt\n",
      "epi_rns.txt\n",
      "epi_tton.txt\n",
      "eskimo.nel\n",
      "exam.50\n",
      "excuse30.txt\n",
      "exidy.txt\n",
      "exylic.txt\n",
      "facedeth.txt\n",
      "failure.txt\n",
      "fajitas.rcp\n",
      "farsi.phrase\n",
      "farsi.txt\n",
      "fartinfo.txt\n",
      "fartting.txt\n",
      "fascist.txt\n",
      "fbipizza.txt\n",
      "fearcola.hum\n",
      "fed.txt\n",
      "fegg!int.txt\n",
      "feggmagi.txt\n",
      "feista01.dip\n",
      "female.jok\n",
      "fiber.txt\n",
      "figure_1.txt\n",
      "final-ex.txt\n",
      "finalexm.hum\n",
      "firecamp.txt\n",
      "fireplacein.txt\n",
      "firstaid.inf\n",
      "firstaid.txt\n",
      "fish.rec\n",
      "flattax.hum\n",
      "flowchrt\n",
      "flowchrt.txt\n",
      "flux_fix.txt\n",
      "focaccia.brd\n",
      "food\n",
      "foodtips\n",
      "footfun.hum\n",
      "forsooth.hum\n",
      "free-cof.fee\n",
      "freshman.hum\n",
      "freudonseuss.txt\n",
      "frogeye1.sal\n",
      "fuck!.txt\n",
      "fuckyou2.txt\n",
      "fudge.txt\n",
      "fusion.gal\n",
      "fusion.sup\n",
      "fwksfun.hum\n",
      "f_tang.txt\n",
      "gack!.txt\n",
      "gaiahuma\n",
      "gameshow.txt\n",
      "ganamembers.txt\n",
      "garlpast.vgn\n",
      "gas.txt\n",
      "gd_drwho.txt\n",
      "gd_flybd.txt\n",
      "gd_frasr.txt\n",
      "gd_gal.txt\n",
      "gd_guide.txt\n",
      "gd_hhead.txt\n",
      "gd_liqtv.txt\n",
      "gd_maxhd.txt\n",
      "gd_ol.txt\n",
      "gd_sgrnd.txt\n",
      "german.aut\n",
      "get.drunk.cheap\n",
      "ghostfun.hum\n",
      "ghostsch.hum\n",
      "gingbeer.txt\n",
      "girlspeak.txt\n",
      "godmonth.txt\n",
      "goforth.hum\n",
      "gohome.hum\n",
      "goldwatr.txt\n",
      "golnar.txt\n",
      "good.txt\n",
      "gotukola.hrb\n",
      "gown.txt\n",
      "grail.txt\n",
      "grammar.jok\n",
      "greenchi.txt\n",
      "grommet.hum\n",
      "grospoem.txt\n",
      "growth.txt\n",
      "gumbo.txt\n",
      "hack\n",
      "hack7.txt\n",
      "hackingcracking.txt\n",
      "hacktest.txt\n",
      "hamburge.nam\n",
      "hangover.txt\n",
      "happyhack.txt\n",
      "harmful.hum\n",
      "hate.hum\n",
      "hbo_spec.rev\n",
      "headlnrs\n",
      "hecomes.jok\n",
      "hedgehog.txt\n",
      "height.txt\n",
      "hell.jok\n",
      "hell.txt\n",
      "herb!.hum\n",
      "hermsys.txt\n",
      "heroic.txt\n",
      "hi.tec\n",
      "hierarch.txt\n",
      "history2.oop\n",
      "hitchcoc.app\n",
      "hitchcok.txt\n",
      "hitler.59\n",
      "hitler.txt\n",
      "hitlerap.txt\n",
      "homermmm.txt\n",
      "hop.faq\n",
      "horflick.txt\n",
      "horoscop.jok\n",
      "horoscop.txt\n",
      "horoscope.txt\n",
      "hotel.txt\n",
      "hotnnot.hum\n",
      "hotpeper.txt\n",
      "how.bugs.breakd\n",
      "how2bgod.txt\n",
      "how2dotv.txt\n",
      "how_to_i.pro\n",
      "htswfren.txt\n",
      "hum2\n",
      "humatra.txt\n",
      "humatran.jok\n",
      "humpty.dumpty\n",
      "iced.tea\n",
      "icm.hum\n",
      "idaho.txt\n",
      "idr2.txt\n",
      "imbecile.txt\n",
      "imprrisk.hum\n",
      "impurmat.hum\n",
      "incarhel.hum\n",
      "indgrdn.txt\n",
      "initials.rid\n",
      "inlaws1.txt\n",
      "inquirer.txt\n",
      "ins1\n",
      "insanity.hum\n",
      "insect1.txt\n",
      "insult\n",
      "insuranc.sty\n",
      "insure.hum\n",
      "interv.hum\n",
      "investi.hum\n",
      "iqtest\n",
      "iremember\n",
      "italoink.txt\n",
      "ivan.hum\n",
      "jac&tuu.hum\n",
      "jalapast.dip\n",
      "jambalay.pol\n",
      "japice.bev\n",
      "japrap.hum\n",
      "jargon.phd\n",
      "jawgumbo.fis\n",
      "jawsalad.fis\n",
      "jayjay.txt\n",
      "jc-elvis.inf\n",
      "jeffie.heh\n",
      "jerky.rcp\n",
      "jimhood.txt\n",
      "johann\n",
      "jokeju07.txt\n",
      "jokes.txt\n",
      "jokes1.txt\n",
      "jon.txt\n",
      "jrrt.riddle\n",
      "jungjuic.bev\n",
      "just2\n",
      "justify\n",
      "kaboom.hum\n",
      "kanalx.txt\n",
      "kashrut.txt\n",
      "kid2\n",
      "kid_diet.txt\n",
      "killer.hum\n",
      "kilroy\n",
      "kilsmur.hum\n",
      "kloo.txt\n",
      "koans.txt\n",
      "labels.txt\n",
      "lampoon.jok\n",
      "languag.jok\n",
      "lansing.txt\n",
      "law.sch\n",
      "lawhunt.txt\n",
      "laws.txt\n",
      "lawskool.txt\n",
      "lawsuniv.hum\n",
      "lawyers.txt\n",
      "lazarus.txt\n",
      "la_times.hun\n",
      "leech.txt\n",
      "legal.hum\n",
      "let.go\n",
      "letgosh.txt\n",
      "letterbx.txt\n",
      "letter_f.sch\n",
      "libraway.txt\n",
      "liceprof.sty\n",
      "lif&love.hum\n",
      "lifeimag.hum\n",
      "lifeinfo.hum\n",
      "lifeonledge.txt\n",
      "limerick.jok\n",
      "lines.jok\n",
      "lion.jok\n",
      "lion.txt\n",
      "lions.cat\n",
      "lipkovits.txt\n",
      "livnware.hum\n",
      "llamas.txt\n",
      "lll.hum\n",
      "llong.hum\n",
      "lobquad.hum\n",
      "looser.hum\n",
      "losers84.hum\n",
      "losers86.hum\n",
      "lost.txt\n",
      "lozers\n",
      "lozeuser.hum\n",
      "lp-assoc.txt\n",
      "lucky.cha\n",
      "ludeinfo.hum\n",
      "ludeinfo.txt\n",
      "luggage.hum\n",
      "luvstory.txt\n",
      "luzerzo2.hum\n",
      "m0dzmen.hum\n",
      "macsfarm.old\n",
      "madhattr.jok\n",
      "madscrib.hum\n",
      "maecenas.hum\n",
      "mailfrag.hum\n",
      "makebeer.hum\n",
      "making_y.wel\n",
      "malechem.txt\n",
      "manager.txt\n",
      "manilla.hum\n",
      "manspace.hum\n",
      "margos.txt\n",
      "marines.hum\n",
      "marriage.hum\n",
      "mash.hum\n",
      "math.1\n",
      "math.2\n",
      "math.far\n",
      "maxheadr\n",
      "mcd.txt\n",
      "mead.rcp\n",
      "meinkamp.hum\n",
      "mel.txt\n",
      "memo.hum\n",
      "memory.hum\n",
      "men&wome.txt\n",
      "mensroom.jok\n",
      "merry.txt\n",
      "miamadvi.hum\n",
      "miami.hum\n",
      "miamimth.txt\n",
      "middle.age\n",
      "minn.txt\n",
      "miranda.hum\n",
      "misery.hum\n",
      "missheav.hum\n",
      "mitch.txt\n",
      "modest.hum\n",
      "modstup\n",
      "mog-history\n",
      "montoys.txt\n",
      "moonshin\n",
      "moore.txt\n",
      "moslem.txt\n",
      "mothers.txt\n",
      "motrbike.jok\n",
      "mov_rail.txt\n",
      "mowers.txt\n",
      "mr.rogers\n",
      "mrscienc.hum\n",
      "mrsfield\n",
      "msfields.txt\n",
      "msorrow\n",
      "mtm.hum\n",
      "mtv.asc\n",
      "murph.jok\n",
      "murphy.txt\n",
      "murphys.txt\n",
      "murphy_l.txt\n",
      "mutate.hum\n",
      "mydaywss.hum\n",
      "myheart.hum\n",
      "naivewiz.hum\n",
      "namaste.txt\n",
      "nameisreo.txt\n",
      "namm\n",
      "nasaglenn.txt\n",
      "necropls.txt\n",
      "netmask.txt\n",
      "netnews.10\n",
      "newcoke.txt\n",
      "newconst.hum\n",
      "newmex.hum\n",
      "news.hum\n",
      "nigel.10\n",
      "nigel.2\n",
      "nigel.4\n",
      "nigel.7\n",
      "nigel10.txt\n",
      "nintendo.jok\n",
      "normal.boy\n",
      "normalboy.txt\n",
      "normquot.txt\n",
      "novel.hum\n",
      "nuke.hum\n",
      "nukewar.jok\n",
      "nukewar.txt\n",
      "nukwaste\n",
      "number\n",
      "number.killer\n",
      "number_k.ill\n",
      "nurds.hum\n",
      "nysucks.hum\n",
      "nzdrinks.txt\n",
      "o-ttalk.hum\n",
      "oakwood.txt\n",
      "oam-001.txt\n",
      "oam.nfo\n",
      "oasis\n",
      "oatbran.rec\n",
      "oculis.rcp\n",
      "odd_to.obs\n",
      "odearakk.hum\n",
      "office.txt\n",
      "ohandre.hum\n",
      "old.txt\n",
      "oldeng.hum\n",
      "oldtime.sng\n",
      "oldtime.txt\n",
      "oliver.txt\n",
      "oliver02.txt\n",
      "onan.txt\n",
      "one.par\n",
      "onetotwo.hum\n",
      "ookpik.hum\n",
      "opinion.hum\n",
      "oranchic.pol\n",
      "orgfrost.bev\n",
      "ourfathr.txt\n",
      "outawork.erl\n",
      "outlimit.txt\n",
      "oxymoron.jok\n",
      "p-law.hum\n",
      "packard.txt\n",
      "paddingurpapers.txt\n",
      "parabl.hum\n",
      "parades.hum\n",
      "parsnip.txt\n",
      "passage.hum\n",
      "passenge.sim\n",
      "pasta001.sal\n",
      "pat.txt\n",
      "pbcookie.des\n",
      "peanuts.txt\n",
      "peatchp.hum\n",
      "pecker.txt\n",
      "penisprt.txt\n",
      "penndtch\n",
      "pepper.txt\n",
      "pepsideg.txt\n",
      "petshop\n",
      "phony.hum\n",
      "phorse.hum\n",
      "phunatdi.ana\n",
      "phxbbs-m.txt\n",
      "pickup.lin\n",
      "pickup.txt\n",
      "pipespec.txt\n",
      "pizzawho.hum\n",
      "planeget.hum\n",
      "planetzero.txt\n",
      "poets.hum\n",
      "pol-corr.txt\n",
      "polemom.txt\n",
      "poli.tics\n",
      "policpig.hum\n",
      "poli_t.ics\n",
      "poll2res.hum\n",
      "polly.txt\n",
      "polly_.new\n",
      "poopie.txt\n",
      "popconc.hum\n",
      "popmach\n",
      "popmusi.hum\n",
      "post.nuc\n",
      "pot.txt\n",
      "potty.txt\n",
      "pournell.spo\n",
      "ppbeer.txt\n",
      "pracjoke.txt\n",
      "prawblim.hum\n",
      "prayer.hum\n",
      "primes.jok\n",
      "princess.brd\n",
      "problem.txt\n",
      "proof.met\n",
      "prooftec.txt\n",
      "proposal.jok\n",
      "proudlyserve.txt\n",
      "prover.wisom\n",
      "prover_w.iso\n",
      "psalm.reagan\n",
      "psalm23.txt\n",
      "psalm_nixon\n",
      "psalm_re.aga\n",
      "psilaine.hum\n",
      "psycho.txt\n",
      "psych_pr.quo\n",
      "pukeprom.jok\n",
      "pun.txt\n",
      "pure.mat\n",
      "puzzle.spo\n",
      "puzzles.jok\n",
      "python_s.ong\n",
      "q.pun\n",
      "qttofu.vgn\n",
      "quantity.001\n",
      "quantum.jok\n",
      "quantum.phy\n",
      "quest.hum\n",
      "quick.jok\n",
      "quotes.bug\n",
      "quotes.jok\n",
      "rabbit.txt\n",
      "racist.net\n",
      "radexposed.txt\n",
      "radiolaf.hum\n",
      "rapmastr.hum\n",
      "ratings.hum\n",
      "ratspit.hum\n",
      "raven.hum\n",
      "reagan.hum\n",
      "realest.txt\n",
      "reasons.txt\n",
      "rec.por\n",
      "recepies.fun\n",
      "recip1.txt\n",
      "recipe.001\n",
      "recipe.002\n",
      "recipe.003\n",
      "recipe.004\n",
      "recipe.005\n",
      "recipe.006\n",
      "recipe.007\n",
      "recipe.008\n",
      "recipe.009\n",
      "recipe.010\n",
      "recipe.011\n",
      "recipe.012\n",
      "reconcil.hum\n",
      "record_.gap\n",
      "red-neck.jks\n",
      "reddwarf.sng\n",
      "reddye.hum\n",
      "relative.ada\n",
      "religion.txt\n",
      "renored.txt\n",
      "renorthr.txt\n",
      "rent-a_cat\n",
      "rentals.hum\n",
      "repair.hum\n",
      "report.hum\n",
      "research.hum\n",
      "residncy.jok\n",
      "resolutn.txt\n",
      "resrch_p.hra\n",
      "resrch_phrase\n",
      "revolt.dj\n",
      "richbred.txt\n",
      "rinaldo.jok\n",
      "rinaldos.law\n",
      "rinaldos.txt\n",
      "ripoffpc.hum\n",
      "rns_bcl.txt\n",
      "rns_bwl.txt\n",
      "rns_ency.txt\n",
      "roach.asc\n",
      "robot.tes\n",
      "rocking.hum\n",
      "rockmus.hum\n",
      "sanshop.txt\n",
      "saveface.hum\n",
      "sawyer.txt\n",
      "seafood.txt\n",
      "seeds42.txt\n",
      "sf-zine.pub\n",
      "sfmovie.txt\n",
      "shameonu.hum\n",
      "shooters.txt\n",
      "shorties.jok\n",
      "shrink.news\n",
      "shuimai.txt\n",
      "shuttleb.hum\n",
      "signatur.jok\n",
      "silverclaws.txt\n",
      "simp.txt\n",
      "sinksub.txt\n",
      "skincat\n",
      "skippy.hum\n",
      "skippy.txt\n",
      "slogans.txt\n",
      "smartass.txt\n",
      "smiley.txt\n",
      "smokers.txt\n",
      "smurf-03.txt\n",
      "smurfs.cc\n",
      "smurf_co.txt\n",
      "snapple.rum\n",
      "snipe.txt\n",
      "soccer.txt\n",
      "socecon.hum\n",
      "social.hum\n",
      "socks.drx\n",
      "solders.hum\n",
      "soleleer.hum\n",
      "solviets.hum\n",
      "soporifi.abs\n",
      "sorority.gir\n",
      "spacever.hum\n",
      "speling.msk\n",
      "spelin_r.ifo\n",
      "spider.hum\n",
      "spoonlis.txt\n",
      "spydust.hum\n",
      "squids.gph\n",
      "staff.txt\n",
      "stagline.txt\n",
      "standard.hum\n",
      "startrek.txt\n",
      "stereo.txt\n",
      "stone.hum\n",
      "strattma.txt\n",
      "stressman.txt\n",
      "studentb.txt\n",
      "st_silic.txt\n",
      "suicide2.txt\n",
      "sungenu.hum\n",
      "supermar.rul\n",
      "swearfrn.hum\n",
      "sw_err.txt\n",
      "symbol.hum\n",
      "sysadmin.txt\n",
      "sysman.txt\n",
      "t-10.hum\n",
      "t-shirt.hum\n",
      "takenote.jok\n",
      "talebeat.hum\n",
      "talkbizr.txt\n",
      "taping.hum\n",
      "teens.txt\n",
      "teevee.hum\n",
      "telecom.q\n",
      "televisi.hum\n",
      "televisi.txt\n",
      "temphell.jok\n",
      "terbear.txt\n",
      "termpoem.txt\n",
      "terms.hum\n",
      "terrmcd'.hum\n",
      "terrnieg.hum\n",
      "test.hum\n",
      "test.jok\n",
      "test2.jok\n",
      "testchri.txt\n",
      "texbeef.txt\n",
      "texican.dic\n",
      "texican.lex\n",
      "textgrap.hum\n",
      "thecube.hum\n",
      "thermite.ana\n",
      "thesis.beh\n",
      "the_ant.txt\n",
      "thievco.txt\n",
      "three.txt\n",
      "tickmoon.hum\n",
      "timetr.hum\n",
      "tnd.1\n",
      "top10.elf\n",
      "top10.txt\n",
      "top10st1.txt\n",
      "top10st2.txt\n",
      "topten.hum\n",
      "toxcwast.hum\n",
      "tpquote2.txt\n",
      "tpquotes.txt\n",
      "transp.txt\n",
      "trekfume.txt\n",
      "trekwes.hum\n",
      "tribble.hum\n",
      "trukdeth.txt\n",
      "truthlsd.hum\n",
      "truths.hum\n",
      "tshirts.jok\n",
      "tuflife.txt\n",
      "tuna.lab\n",
      "turbo.hum\n",
      "turing.shr\n",
      "turkey.fun\n",
      "twilight.txt\n",
      "twinkie.txt\n",
      "twinkies.jok\n",
      "twinpeak.txt\n",
      "t_zone.jok\n",
      "un.happy\n",
      "units.mea\n",
      "univ.odd\n",
      "unochili.txt\n",
      "vaguemag.90s\n",
      "valujet.txt\n",
      "variety2.asc\n",
      "variety3.asc\n",
      "vegan.rcp\n",
      "vegkill.txt\n",
      "venganza.txt\n",
      "venison.txt\n",
      "voltron.hum\n",
      "vonthomp\n",
      "wacky.ani\n",
      "wagit.txt\n",
      "wagon.hum\n",
      "waitress.txt\n",
      "washroom.txt\n",
      "watchlip.hum\n",
      "wedding.hum\n",
      "weight.txt\n",
      "weights.hum\n",
      "welfare\n",
      "welfare.txt\n",
      "wetdream.hum\n",
      "whatbbs\n",
      "whatthe.hum\n",
      "whitbred.txt\n",
      "who.txt\n",
      "whoon1st.hum\n",
      "whoops.hum\n",
      "why-me.hum\n",
      "widows\n",
      "wimptest.txt\n",
      "wisdom\n",
      "wkrp.epi\n",
      "women.jok\n",
      "wonton.txt\n",
      "wood\n",
      "woodbine.txt\n",
      "woodbugs.txt\n",
      "woods.txt\n",
      "woodsmok.txt\n",
      "woolly_m.amm\n",
      "word.hum\n",
      "worldend.hum\n",
      "wrdnws1.txt\n",
      "wrdnws2.txt\n",
      "wrdnws4.txt\n",
      "wrdnws5.txt\n",
      "wrdnws8.txt\n",
      "wrdnws9.txt\n",
      "x-drinks.txt\n",
      "xtermin8.hum\n",
      "y.txt\n",
      "yjohncse.hum\n",
      "yogisays.txt\n",
      "yogurt.asc\n",
      "yuban.txt\n",
      "yuppies.hum\n",
      "zen.txt\n",
      "zgtoilet.txt\n",
      "zodiac.hum\n",
      "zucantom.sal\n",
      "zuccmush.sal\n"
     ]
    }
   ],
   "source": [
    "result = process(input_query, input_ops)\n",
    "print('\\nNumber of documents matched:', len(result[1]))\n",
    "print('Number of comparisons:', result[0])\n",
    "print('\\nDocument list\\n')\n",
    "\n",
    "for docid in result[1]:\n",
    "    print(docnames[docid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc71775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0125e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
